\section{Introduction}
\thispagestyle{plain} % surpress header on first page

Economics has been finding interest in questions of cause and effect ever since the classical period in which Adam Smith wrote his monumental "An Inquiry into the Nature and Causes of the Wealth of Nations" in 1776 more commonly known as simply "The Wealth of Nations". Noting that "cause" is already hidden in this classic's title, \cite{Hoover.2008} continues by finding traces of causality in the works of David Hume in the 18th and John Stuart Mill in the 19th century that still remain in contemporary approaches to causality in Economics. Following \cite{Heckman.2013} it has not been until Trygve Haavelmo in the 1940's, though, that causality in Economics was set onto a rigorous foundation (compare \cite{Haavelmo.1943, Haavelmo.1944}). While \cite{Hoover.2008} reports that causal vocabulary essentially disappeared from Economics in the time between 1950 and 1990, it found a revival in the course of the 1990s which was partly due to an increasing interest in structural models in applied Microeconomics. Prominent examples from Industrial Organization are \cite{Berry.1995} and \cite{Goldberg.1995}, \cite{Keane.1994} and \cite{Keane.1997} in Labour Economics as well as the paper this thesis makes heavy use of: \cite{Rust.1987}. \cite{Low.2017} define a structural econometric model by its close tie to a theoretical economic framework. The idea is to identify the endogenous structural parameters of the model with data. By establishing an exhaustive mechanism (theoretical model) paired with the accuracy of its assumptions, a causal effect can potentially be identified (compare \cite{Morgan.2015}). The main advantage of this approach as opposed to approaches that do not rest on Economic theory explicitly is already reported in 1954 by \cite{Haavelmo.2015} (published post mortem) which is to "study the functioning of alternative structures that could have practical interest from the point of view of economic reform". The approach allows for counterfactual analysis for which there is no or not enough data and the model as well as its counterfactual implications can be tested (\cite{Low.2017}). Another advantage is that they make the identifying assumptions of the model explicit as noted by \cite{Keane.2010}. At the same time, though, Keane describes that the amount of assumptions usually needed combined with the computational complexity pose large challenges to structural modeling. While observing a decline in the use of the structural approach in Econometrics, he identifies the need for serious model validation which involves testing the fit of the model in as well as out-of sample to make the approach more successful and appealing. \cite{Rust.2014} and \cite{Wolpin.2013} agree on this improvement strategy while \cite{Angrist.2010} go beyond that. They argue that one driver of success is sensitivity analysis as already requested by \cite{Leamer.1983} in which "researchers show how their results vary with changes in specification or functional form". While the authors are pleased with the developments in this area, \cite{Harenberg.2019} draw a different picture for Economics in this regard who deem the current practice "subjective" and the derived judgments "somewhat arbitrary". Based on this they advocate a more systematic approach to sensitivity analysis that has been developed as part of the applied mathematics field of Uncertainty Quantification (UQ).

The general idea behind this field is to argue that scientific modeling (such as done in structural Econometrics) comes with many potential errors stemming from e.g. the model itself or its numerical implementation which potentially affects the accuracy of its predictions (\cite{Smith.2013}). As argued by \cite{Keane.2010} and \cite{Rust.2010}, the most important criterion of quality for structural econometric models is its fit on the data at hand as well as its out of sample predictions. This indicates that a serious quantification of the effects of the errors on the predictions is very important in the context of structural Econometrics. Another factor to promote a systematic approach to uncertainty quantification is the credibility and reliability of science from a policy perspective and that of its general audience especially in the times of fake news, climate change and Covid-19 as recently highlighted by several authors (see \cite{Fischhoff.2014} \cite{DeBruin.2019}, \cite{VanDerBles.2019}, \cite{Manski.2019} and \cite{Manski.2020}). In the light of uncertainty quantification, in my thesis, I revisit the structural model of optimal bus engine replacement pioneered by \cite{Rust.1987} and perform a sensitivity analysis of the model as envisioned by \cite{Leamer.1983}.

The main contribution of Rust's paper is his novel approach to calibrating model parameters by the Nested Fixed Point Algorithm (NFXP). While ever since many other estimation techniques for single agent dynamic discrete choice models such as the Rust model have been introduced, most of them are not full solution methods (i.e. not at every structural parameter guess the economic model is solved) as reported in \cite{Aguirregabiri.2010}. 25 years later, \cite{Su.Judd.2012} suggest a new full solution method for the Rust model claiming that it is superior to its predecessor introduced by Rust. They call the approach Mathematical Programming with Equilibrium Constraints (MPEC). Henceforth many applied studies especially in the field of Industrial Organization have applied MPEC for calibration of structural models (see e.g. \cite{Aryal.2013}, \cite{Kaiser.2014}, \cite{Hubbard.2009}, \cite{Reynaert.2014}, \cite{Freyberger.2015}). But only a few studies compare the performance of the NFXP to MPEC which include \cite{Su.Judd.2012}, \cite{Dube.Fox.Su.2012}, \cite{Iskhakov.2016} and \cite{Dong.Hsieh.Zhang.2017}. All of them, mainly compare the two approaches along a quantitative dimension (speed, rate of convergence, etc.) within well-behaved settings without deliberate misspecification or other considerable error.

The setting in my thesis deviates from the previous studies in this regard combining the comparison of NFXP and MPEC additionally with tools from the UQ literature. I run a Monte Carlo simulation based on the Rust model for a certain true model specification. In the following, I estimate the model parameters for different specifications that depart from the true underlying data generating process in several ways. I allow for misspecification of the cost function and the discount factor in the model dimension. Further I incorporate numerical error by estimating the model using numerical first order derivatives and lower grid size when discretizing the expected value function than actually used to generate the data. For all possible combinations of these errors I estimate the model parameters with NFXP and MPEC, respectively, for each simulated data set. Per specification I obtain a parameter distribution. In order to compare MPEC and NFXP qualitatively across different specifications, the tool from UQ, uncertainty propagation, lends itself perfectly. Within a parameter distribution, for every single parameter vector the model is used to derive a counterfactual model prediction (in my case a counterfactual demand level). This results in a distribution of this counterfactual prediction per specification which is, as noted before, a qualitative attribute along which MPEC and NFXP can be compared. Additionally, I compare the two approaches also with quantitative measures as done in previous studies.

Comparing the resulting distributions of the model prediction to the one obtained from calibrating the model with the correct model and numerical specification reveals that irrespective of MPEC or NFXP, small deviations from the correct specification have a considerable impact. This finding is visualized in the spirit of UQ (following \cite{Oberkampf.2010}) and communicated such that it is informative to policy-makers. In the comparison of MPEC and NFXP I obtain remarkable differences in the model predictions when allowing for misspecification of the cost function. This is surprising as, in theory, both should arrive at the same solution but here the specific differences in numerical implementation seem to prevent them from doing so. In combination with the idea of a systematic sensitivity analysis of model calibrations, this suggests that as opposed to how it has been promoted by \cite{Su.Judd.2012} and practiced by those using MPEC, NFXP and MPEC should be seen as complements rather than competitors - both in combination playing a role in making policy recommendations and model predictions more robust.

The structure of my thesis is the following. I introduce Mathematical Programming with Equilibrium Constraints in a general framework based on \cite{Su.Judd.2012} in section \ref{generalMPEC}. In section \ref{rustmodel}, the Rust model based on \cite{Rust.1987} is explained and the theory of MPEC is applied to it. I conclude that section with a validation of my programming code for MPEC by replicating \cite{Iskhakov.2016}. Section \ref{uq} explains the Uncertainty Quantification framework as well as its terminology. Further I apply uncertainty propagation in the Rust model by quantifying the uncertainty around the demand function derived from the Rust model. In section \ref{sensitivity}, I present my Monte Carlo simulation as well as its results. In the last section, number \ref{conclusion}, I draw conclusions from my presented results.

