\section{Uncertainty Quantification}
\thispagestyle{plain} % surpress header on first page

Most sciences work at least to a certain degree quantitatively and by doing so they often rely on theoretical models that in many cases approximate the underlying true mechanisms that drive a certain process or phenomenon. In many cases the quantitative model outcome of interest is the result of a transformation of model inputs through a complex mathematical system. Across disciplines, from a model perspective many inputs and initial states as well as their development are not deterministic but are uncertain. The model inputs are often informed by theory, previous quantitative studies as well as experiments or need to be calibrated using limited data but cannot be treated as known. It is now often argued that the inherently uncertain parameters propagate their error through the model which yields an uncertain model outcome. In order to make informed decisions based on the model outcome, this uncertainty should be quantified and communicated. This constitutes one main motivation why the applied mathematics field of Uncertainty Quantification (UQ) developed and still is an active field of research. Many of its advancements come from the engineering and physical sciences in which negligence of uncertainty in the outcomes can lead to attributable and severe consequences. In climatology, the predicted path of a hurricane is reported with uncertainties which are due to the dynamical, complex weather conditions that can only be approximated. These uncertainties inform countries about whether they might be hit while giving them valuable time to prepare. An example from engineering involves the safety of a nuclear power plant which in turn is affected by factors such as the weather and its environment in general. This also includes seismological activity which was causing the catastrophe in Fukushima back in 2013. Uncertainty Quantification tries to incorporate those risks in the modeling process and make them explicit (compare \cite{Sullivan.2015} and \cite{Smith.2013}).

Taking a step back from those very precise motivations for UQ, \cite{Smith.2019} make a simple mathematical point suggesting that it is worth to investigate uncertainty. Their motivation rests on the butterfly effect (see \cite{Lorenz.1963}) which demonstrates that already slight differences in initial conditions can result in an unforeseeable change in the outcome of a complex dynamical system over time. Going back to our initial observation that models often are complex mathematical systems, this provokes the thought that also in this case small changes in inputs might have a strong impact on model predictions. But how does that matter in Economics? And how do we quantify this uncertainty? An answer to the first question and a motivation for UQ can be found in a recent study by \cite{Cai.2019}. They estimate "the social cost of carbon (SCC), defined as the marginal economic loss caused by an extra metric ton of atmospheric carbon" \footnote{Compare page 3 in \cite{Cai.2019}.} based on the Dynamic Integrated Climate–Economy” (DICE) model (\cite{Nordhaus.1992, Nordhaus.2008}). They add uncertainty regarding the economic development as well as the change in climate conditions to this framework by assuming possible ranges of those factors and propagating them through their model. The result is quite astonishing as for the year 2005 the SCC varies between 59 to 99 Dollars per ton of carbon for a realistic range of projections in economic and climatic outlook. This range of uncertainty in the model outcome is quite large but clearly sends a more informative message to policy makers.

While it is well understood in Economics that there is uncertainty affecting model outcomes and hence policy recommendations as well as that its communication is crucial, it is rarely the case that a full, coherent evaluation of uncertainties is performed that goes beyond robustness checks (see \cite{Manski.2019} and \cite{Scheidegger.2019}). This is where the already existent UQ framework can come in and answer the second question from before, how to quantify uncertainty and determine which sources are contributing to it as recently done in \cite{Scheidegger.2019} and in \cite{Harenberg.2019}. In the next part, I will introduce the cornerstones of this framework and apply some of its general ideas to the Rust model and the comparison of MPEC and the NFXP.

\subsection{The UQ Framework}

For the following description I mainly rely on \cite{Smith.2013} who outlines a comprehensive UQ framework with a focus on engineering and physics as well as \cite{Oberkampf.2010} who deal with uncertainty in more general scientific computing context.  The starting point for both is the observation that there are many potential sources of uncertainty that can affect the outcome of a computational model. First of there is model uncertainty which translates into the underlying mathematical framework of a model. In many cases the true underlying process that the mathematical model is supposed to capture is not perfectly understood which renders the mathematical representation imprecise. In the Rust Model we impose the behavioral assumption on Harold Zurcher that his decision process follows that of a maximization of his discounted life time utility over an infinite horizon. We do not know whether this is warranted and even if it was, on a lower level there is still uncertainty about how his underlying cost function exactly looks like. Quite obviously the choice of the specific mathematical representation affects the predictions the model will make after being calibrated. A second source are the parameter inputs themselves. As in Economics they usually are estimated from data, there always remains uncertainty around the specific parameter estimates. This parameter uncertainty translates into uncertainty of the model outcomes. Thirdly, in many applications the computational implementation of a mathematical model involves approximate numerical solutions to the whole model itself or at least parts of it. In our example, we rely on numerical optimization algorithms that introduce some error but also the discretization of the expected value function is solely an approximation of the true mathematical relation. A last factor is a potential measurement error in the model inputs. This could be for instance an imprecise measurement of the cumulative mileage state of some buses.

The authors argue now that the very nature of those uncertainties can be different. On the one hand, the uncertainty can be \textit{aleatoric} which means that it is stochastic and cannot be reduced by gaining additional economic or experimental knowledge. For those uncertainties there typically exists a probability distribution. On the other hand, there is \textit{epistemic} uncertainty which can generally be reduced. It includes for example the previously stated uncertainty of the correct cost function which might be solved by running an experiment. For this example, there clearly is no probability distribution describing the error but a rather an interval of different possible uncertainties. Given that some of these uncertainties are uncovered in a specific model, the UQ framework prescribes the following two steps. Before we go into this, let us first refine the terms of model and model outcome. In the UQ framework, the computational model as in our Rust Model can be described like this:

\begin{equation*}
	y = f(\theta)
\end{equation*}

where $\theta$ are the model inputs or parameters and $y \in \mathbb{R}^Q$ the model output or the quantity of interest (QoI). \paragraph{}
The QoI can be any policy relevant vector or scalar that could be obtained from the underlying mathematical model. In \cite{Cai.2019} this is the social cost of carbon. In my thesis, this will be some counterfactual demand level of bus engines given a certain replacement cost. I will further go into detail about this in the next section. This computational model itself can already suffer from discrepancies to the true mathematical model due to model errors and numerical errors. Given that the model parameters have to be calibrated from data and hence suffer from uncertainty in the estimates, they are represented by a random vector $\Theta$ which has a certain probability distribution depending on the data at hand. Given the calibrated parameters, the uncertainty in the parameters affect the model outcome in the following way:

\begin{equation*}
	Y = f(\Theta)
\end{equation*}

with $Y$ being the QoI that itself is a random vector or variable. \paragraph{}

The fact that measurement, numerical and model uncertainty might exist is implicitly included in the fact that the computational model $f(.)$ is not equivalent to the true mathematical model, i.e. the QoI $y$ and the parameter vector $\theta$ are already not deterministic but rather uncertain affected by the above mentioned errors.

Coming back to our two steps in the UQ framework, first, the uncertainties are accounted for in the model inputs and then propagated through the model. Second, this propagation is then accounted for in some uncertainty around the quantity of interest. In a sub field of UQ, sensitivity analysis, this propagation technique is exploited to determine which parameters of the model contribute the most to the observed uncertainty in the QoI. This can be of great interest even beyond the argument of informed policy decisions as it can given valuable information to researchers that want to calibrate a certain economic model with actual data. It gives them an indication on which parameters to focus on during the calibration procedure (e.g. choose the stopping tolerance of an optimization algorithm such that it gives the most accurate estimate of a certain parameter that drives the uncertainty while loosening the tolerance for other parameters that are less importatn) which in turn might reduce the uncertainty in the QoI (see \cite{Scheidegger.2019},  \cite{Harenberg.2019} and \cite{Ghanem.2017}). In this literature, usually a host of combinations of different parametrizations is used to propagate through the model and calculate its QoI. This information from several runs is then used to systematically determine the influence of the uncertainty in certain parameters on the uncertainty in the QoI. With complexity of the computational model, this becomes unfeasible as it would involve too many runs of the model. This is the reason why the literature developed so called surrogate models that approximate the true computational model while maintaining higher speed of convergence for a single run (compare also \cite{Saltelli.2008}). While this strand of literature focuses on parameter uncertainty, in the setting of economic models it also implicitly takes some other forms of uncertainty from the calibration procedure into account as the exact estimate of a parameter depends also on model and numerical specifications in the calibration procedure.

In the following section I will introduce the mathematical and computational model for the QoI from which the remainder of my thesis will draw.

\subsection{The Quantity of Interest: The Demand Function}

